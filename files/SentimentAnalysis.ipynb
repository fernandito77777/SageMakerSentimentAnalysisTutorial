{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text Classification can be used to solve various use-cases like sentiment analysis, spam detection, hashtag prediction etc. This notebook demonstrates the use of SageMaker BlazingText to perform supervised binary/multi class with single or multi label text classification. BlazingText can train the model on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. BlazingText extends the fastText text classifier to leverage GPU acceleration using custom CUDA kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::930652504693:role/service-role/AmazonSageMaker-ExecutionRole-20210608T154827\n",
      "sagemaker-ap-southeast-1-930652504693\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = \"blazingtext/supervised\"  # Replace with the prefix under which you want to store the data if needed\n",
    "\n",
    "data_bucket = \"fnandito-ml-sentiment-data\"  # Replace with the bucket where your data is located\n",
    "data_prefix = \"LabelledData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering\n",
    "\n",
    "First, let's take a look at our data. We'll use the data from S3 bucket we have uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = \"IndonesianTweetLabelled\"\n",
    "DataLocation = data_prefix+\"/\"+FileName\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.download_file(Bucket=data_bucket, Key=DataLocation, Filename=FileName) #Download S3 file to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>lagu bosan apa yang aku save ni huhuhuhuhuhuhu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>kita lanjutkan saja diam ini hingga kau dan ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doa rezeki tak putus inna haa zaa larizquna ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>makasih loh ntar kita bagi hasil aku 99 9 sisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>aku tak faham betul jenis orang malaysia yang ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen                                              Tweet\n",
       "0        -1  lagu bosan apa yang aku save ni huhuhuhuhuhuhu...\n",
       "1        -1  kita lanjutkan saja diam ini hingga kau dan ak...\n",
       "2         1  doa rezeki tak putus inna haa zaa larizquna ma...\n",
       "3         1  makasih loh ntar kita bagi hasil aku 99 9 sisa...\n",
       "4        -1  aku tak faham betul jenis orang malaysia yang ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Data = pd.read_csv(FileName, sep='\\t', header=0)\n",
    "Data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText expects a single preprocessed text file with space separated tokens and each line of the file should contain a single sentence and the corresponding label(s) prefixed by \"\\___label\\___\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PositiveData = []\n",
    "NeutralData = []\n",
    "NegativeData = []\n",
    "\n",
    "for index, row in Data.iterrows():\n",
    "    if row['sentimen'] == 1:\n",
    "        PositiveData.append(\"__label__Positive \" + row['Tweet'])\n",
    "    elif row['sentimen'] == 0:\n",
    "        NeutralData.append(\"__label__Neutral \" + row['Tweet'])\n",
    "    elif row['sentimen'] == -1:\n",
    "        NegativeData.append(\"__label__Negative \" + row['Tweet'])\n",
    "    else:\n",
    "        print(\"Mislabelled data detected on index\" + index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much the data we have for each of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "LabelName = ['Positive', 'Neutral', 'Negative']\n",
    "TotalLabel = [len(PositiveData), len(NeutralData), len(NegativeData)]\n",
    "plt.barh(LabelName, TotalLabel)\n",
    "  \n",
    "for index, value in enumerate(TotalLabel):\n",
    "    plt.text(value, index,\n",
    "             str(value))\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your file and upload to S3 so it can be recognized by the algorithm. Need to split the data as well. To split it, We are going to use 80-20 split. Since we have splitted it based on labels, we are going to split the label, and merge it into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.shuffle(PositiveData)\n",
    "random.shuffle(NeutralData)\n",
    "random.shuffle(NegativeData)\n",
    "\n",
    "PositiveNArr = np.array(PositiveData)\n",
    "NeutralNArr = np.array(NeutralData)\n",
    "NegativeNArr = np.array(NegativeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PositiveTrain = PositiveNArr[:int(len(PositiveNArr)*0.8)]\n",
    "PositiveTest = PositiveNArr[-int(len(PositiveNArr)*0.2):]\n",
    "NeutralTrain = NeutralNArr[:int(len(NeutralNArr)*0.8)]\n",
    "NeutralTest = NeutralNArr[-int(len(NeutralNArr)*0.2):]\n",
    "NegativeTrain = NegativeNArr[:int(len(NegativeNArr)*0.8)]\n",
    "NegativeTest = NegativeNArr[-int(len(NegativeNArr)*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = np.array(PositiveTrain)\n",
    "TrainData = np.append(TrainData, NeutralTrain)\n",
    "TrainData = np.append(TrainData, NegativeTrain)\n",
    "\n",
    "TestData = np.array(PositiveTest)\n",
    "TestData = np.append(TestData, NeutralTest)\n",
    "TestData = np.append(TestData, NegativeTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save the train and test to S3 that is being used for sagemaker, so it can be accessed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from numpy import savetxt\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "\n",
    "TrainFileName = \"TrainData-\" + timestamp\n",
    "TestFileName = \"TestData-\" + timestamp\n",
    "\n",
    "np.savetxt(TrainFileName, TrainData , fmt='%s')\n",
    "np.savetxt(TestFileName, TestData, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"FormattedData/SentimentAnalysis\"\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=TrainFileName, bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=TestFileName, bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.remove(TrainFileName)\n",
    "os.remove(TestFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 475088953585.dkr.ecr.ap-southeast-1.amazonaws.com/blazingtext:1 (ap-southeast-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for supervised text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the original implementation of [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf), SageMaker BlazingText provides an efficient implementation of the continuous bag-of-words (CBOW) and skip-gram architectures using Negative Sampling, on CPUs and additionally on GPU[s]. The GPU implementation uses highly optimized CUDA kernels. To learn more, please refer to [*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs*](https://dl.acm.org/citation.cfm?doid=3146347.3146354).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides skip-gram and CBOW, SageMaker BlazingText also supports the \"Batch Skipgram\" mode, which uses efficient mini-batching and matrix-matrix operations ([BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)). This mode enables distributed word2vec training across multiple CPU nodes, allowing almost linear scale up of word2vec computation to process hundreds of millions of words per second. Please refer to [*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText also supports a *supervised* mode for text classification. It extends the FastText text classifier to leverage GPU acceleration using custom CUDA kernels. The model can be trained on more than a billion words in a couple of minutes using a multi-core CPU or a GPU, while achieving performance on par with the state-of-the-art deep learning text classification algorithms. For more information, please refer to the [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the following modes are supported by BlazingText on different types instances:\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|                \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|      \t|          \t|        ✔       \t|     | |\n",
    "\n",
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `c4.4xlarge` instance.\n",
    "\n",
    "Refer to [BlazingText Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) in the Amazon SageMaker documentation for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=35,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 5,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"vector_dim\": 10,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 4,\n",
    "        \"min_epochs\": 5,\n",
    "        \"word_ngrams\": 3,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `Estimator` classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. Therefore it might be a few minutes before we start getting training logs for our training jobs. The data logs will also print out Accuracy on the validation data for every epoch after training job has executed `min_epochs`. This metric is a proxy for the quality of the algorithm. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-12 06:00:48 Starting - Starting the training job...\n",
      "2021-07-12 06:00:50 Starting - Launching requested ML instancesProfilerReport-1626069648: InProgress\n",
      "......\n",
      "2021-07-12 06:02:15 Starting - Preparing the instances for training......\n",
      "2021-07-12 06:03:15 Downloading - Downloading input data...\n",
      "2021-07-12 06:03:45 Training - Training image download completed. Training in progress.\n",
      "2021-07-12 06:03:45 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 WARNING 140066778305920] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 WARNING 140066778305920] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] nvidia-smi took: 0.02518773078918457 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] 6 files found in train channel. Using /opt/ml/input/data/train/TrainData-12-07-2021-06:00:47 for training...\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] Processing /opt/ml/input/data/train/TrainData-12-07-2021-06:00:47 . File size: 0.8235006332397461 MB\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] 6 files found in validation channel. Using /opt/ml/input/data/validation/TestData-12-07-2021-02:34:46 for training...\u001b[0m\n",
      "\u001b[34m[07/12/2021 06:03:35 INFO 140066778305920] Processing /opt/ml/input/data/validation/TestData-12-07-2021-02:34:46 . File size: 0.20574283599853516 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  8242\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/TestData-12-07-2021-02:34:46\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.634722\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 7.95 #####\n",
      "\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 7.95\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 0.09\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.6559\u001b[0m\n",
      "\u001b[34mNumber of train examples: 8643\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.6347\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 2160\u001b[0m\n",
      "\n",
      "2021-07-12 06:04:15 Completed - Training job completed\n",
      "Training seconds: 49\n",
      "Billable seconds: 49\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Positive\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.38888469338417053\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Neutral\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.7176376581192017\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Negative\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.4486387372016907\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentences = [\n",
    "    \"saya bahagia dan senang atas kerja keras anda\",\n",
    "    \"saya ragu dalam mengambil keputusan\",\n",
    "    \"Saya tidak setuju dengan pendapat anda\",\n",
    "]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the model will return only one prediction, the one with the highest probability. For retrieving the top k predictions, you can set `k` in the configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.38888469338417053,\n",
      "      0.3500923216342926\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Neutral\",\n",
      "      \"__label__Negative\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.7176376581192017,\n",
      "      0.14643968641757965\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.4486387372016907,\n",
      "      0.3628771901130676\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization (HPO)\n",
    "*Note, with the default settings below, the hyperparameter tuning job can take up to ~20 minutes to complete.*\n",
    "\n",
    "We will use SageMaker HyperParameter Optimization (HPO) to automate the searching process effectively. Specifically, we **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required HPO objects\n",
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    "    IntegerParameter\n",
    ")\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    #\"mode\": CategoricalParameter(\"supervised\"),\n",
    "    \"epochs\": IntegerParameter(1, 5),\n",
    "    \"learning_rate\": ContinuousParameter(0.01, 0.2),\n",
    "    \"min_count\": IntegerParameter(1, 5),\n",
    "    \"word_ngrams\": IntegerParameter(1, 3),\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:accuracy\"\n",
    "\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=bt_model,  # the SageMaker estimator object\n",
    "    hyperparameter_ranges=ranges,  # the range of hyperparameters\n",
    "    max_jobs=10,  # total number of HPO jobs\n",
    "    max_parallel_jobs=5,  # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",  # the internal optimization strategy of HPO\n",
    "    objective_metric_name=objective,  # the objective metric to be used for HPO\n",
    "    objective_type=\"Maximize\",  # maximize or minimize the objective metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# start HPO\n",
    "tuner.fit(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy HPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-07-12 06:18:49 Starting - Preparing the instances for training\n",
      "2021-07-12 06:18:49 Downloading - Downloading input data\n",
      "2021-07-12 06:18:49 Training - Training image download completed. Training in progress.\n",
      "2021-07-12 06:18:49 Uploading - Uploading generated training model\n",
      "2021-07-12 06:18:49 Completed - Training job completed\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "# deploy the best model from HPO\n",
    "hpo_predictor = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Positive\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.6487286686897278\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Neutral\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.9901147484779358\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Negative\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.7845661640167236\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "['saya bahagia dan senang atas kerja keras anda', 'saya ragu dalam mengambil keputusan', 'Saya tidak setuju dengan pendapat anda']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"saya bahagia dan senang atas kerja keras anda\",\n",
    "    \"saya ragu dalam mengambil keputusan\",\n",
    "    \"Saya tidak setuju dengan pendapat anda\",\n",
    "]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences}\n",
    "\n",
    "response = hpo_predictor.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.6487286686897278,\n",
      "      0.3419760763645172\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Neutral\",\n",
      "      \"__label__Positive\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.9901147484779358,\n",
      "      0.005970051046460867\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ],\n",
      "    \"prob\": [\n",
      "      0.7845661640167236,\n",
      "      0.15173567831516266\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = hpo_predictor.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop / Close the Endpoint (Optional)\n",
    "Finally, we should delete the endpoint before we close the notebook if we don't need to keep the endpoint running for serving realtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(text_classifier.endpoint)\n",
    "sess.delete_endpoint(hpo_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
